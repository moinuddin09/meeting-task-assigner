{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRjZdPJtrZJc",
        "outputId": "00077c15-8941-40bc-ddf1-9c641067795f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project folder: /content/drive/MyDrive/meeting-task-assigner\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "BASE = \"/content/drive/MyDrive/meeting-task-assigner\"\n",
        "os.makedirs(BASE, exist_ok=True)\n",
        "print(\"Project folder:\", BASE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!df -h | sed -n '1,6p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb78bmDDrt5n",
        "outputId": "b4a8e9f6-a3e8-4fa8-cac4-0cc840697cc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   39G   70G  36% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n",
            "/dev/sda1        73G   40G   34G  54% /kaggle/input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab shell cell\n",
        "!apt-get update -y && apt-get install -y ffmpeg\n",
        "!pip install --upgrade pip\n",
        "!pip install openai-whisper pydub spacy dateparser fuzzywuzzy python-Levenshtein pytest tqdm\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw3L3z2xrwpX",
        "outputId": "3592c510-0c00-49f7-991e-65790c5a6c5d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.22)] [Waiting for headers] [1 I\r0% [Connecting to archive.ubuntu.com (91.189.92.22)] [Waiting for headers] [Con\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,835 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n",
            "Fetched 37.5 MB in 6s (6,750 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (8.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.11.3)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (5.3.1)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest) (2.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=de6600ac239b613731ba996df6e85b5ccd17239b7ebcc58e3853e62decf7e31e\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, dateparser, python-Levenshtein, openai-whisper\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [openai-whisper]\n",
            "\u001b[1A\u001b[2KSuccessfully installed Levenshtein-0.27.3 dateparser-1.2.2 fuzzywuzzy-0.18.0 openai-whisper-20250625 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, textwrap\n",
        "BASE = \"/content/drive/MyDrive/meeting-task-assigner\"\n",
        "folders = [\"src\",\"sample_data\",\"tests\",\"outputs\"]\n",
        "for f in folders:\n",
        "    os.makedirs(os.path.join(BASE,f), exist_ok=True)\n",
        "\n",
        "# Create placeholder files\n",
        "files = {\n",
        "    \"README.md\": \"# Meeting Task Assigner\\n\\nProject workspace in Google Drive.\",\n",
        "    \"requirements.txt\": \"openai-whisper\\npydub\\nspacy\\ndateparser\\nfuzzywuzzy\\npython-Levenshtein\\npytest\\ntqdm\\n\",\n",
        "    \"src/transcribe.py\": \"# placeholder\",\n",
        "    \"src/nlp_pipeline.py\": \"# placeholder\",\n",
        "    \"src/assigner.py\": \"# placeholder\",\n",
        "    \"sample_data/team.json\": \"\"\"[\n",
        "  {\"name\":\"Sakshi\", \"role\": \"Frontend Developer\", \"skills\":[\"React\",\"JavaScript\",\"UI\",\"frontend\",\"ui bugs\"]},\n",
        "  {\"name\":\"Mohit\", \"role\": \"Backend Engineer\", \"skills\":[\"Database\",\"APIs\",\"Performance\",\"backend\",\"optimization\"]},\n",
        "  {\"name\":\"Arjun\", \"role\": \"UI/UX Designer\", \"skills\":[\"Figma\",\"User flows\",\"Mobile design\",\"UI\",\"UX\"]},\n",
        "  {\"name\":\"Lata\", \"role\": \"QA Engineer\", \"skills\":[\"Testing\",\"Automation\",\"Quality assurance\",\"unit tests\",\"QA\"]}\n",
        "]\"\"\"\n",
        "}\n",
        "for path, content in files.items():\n",
        "    p = os.path.join(BASE, path)\n",
        "    d = os.path.dirname(p)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as fh:\n",
        "        fh.write(content)\n",
        "print(\"Project skeleton created at\", BASE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgySjRjHr0uW",
        "outputId": "ee60b3b2-b2c7-4741-b35f-fcdee1f009bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project skeleton created at /content/drive/MyDrive/meeting-task-assigner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib, sys, os\n",
        "modules = [\"whisper\",\"pydub\",\"spacy\",\"dateparser\",\"fuzzywuzzy\",\"Levenshtein\"]\n",
        "for m in modules:\n",
        "    try:\n",
        "        importlib.import_module(m)\n",
        "        print(m, \"OK\")\n",
        "    except Exception as e:\n",
        "        print(m, \"FAILED:\", e)\n",
        "\n",
        "# show created files\n",
        "!ls -la \"/content/drive/MyDrive/meeting-task-assigner\"\n",
        "!ls -la \"/content/drive/MyDrive/meeting-task-assigner/src\"\n",
        "!ls -la \"/content/drive/MyDrive/meeting-task-assigner/sample_data\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wtiM0Krsh7N",
        "outputId": "66a22c25-a87d-49af-9f81-7a7ce3cd6c25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisper OK\n",
            "pydub OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spacy OK\n",
            "dateparser OK\n",
            "fuzzywuzzy OK\n",
            "Levenshtein OK\n",
            "total 17\n",
            "drwx------ 2 root root 4096 Nov 29 03:39 outputs\n",
            "-rw------- 1 root root   59 Nov 29 03:39 README.md\n",
            "-rw------- 1 root root   80 Nov 29 03:39 requirements.txt\n",
            "drwx------ 2 root root 4096 Nov 29 03:39 sample_data\n",
            "drwx------ 2 root root 4096 Nov 29 03:39 src\n",
            "drwx------ 2 root root 4096 Nov 29 03:39 tests\n",
            "total 2\n",
            "-rw------- 1 root root 13 Nov 29 03:39 assigner.py\n",
            "-rw------- 1 root root 13 Nov 29 03:39 nlp_pipeline.py\n",
            "-rw------- 1 root root 13 Nov 29 03:39 transcribe.py\n",
            "total 1\n",
            "-rw------- 1 root root 449 Nov 29 03:39 team.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a synthetic meeting audio using gTTS (Google Text-to-Speech)\n",
        "!pip install gTTS --quiet\n",
        "\n",
        "from gtts import gTTS\n",
        "import os, textwrap\n",
        "\n",
        "meeting_text = textwrap.dedent(\"\"\"\n",
        "Hi everyone, let's discuss this week's priorities. Sakshi, we need someone to fix the critical login bug that users reported yesterday. This needs to be done by tomorrow evening since it's blocking users. Also, the database performance is really slow, Mohit you're good with backend optimization right? We should tackle this by end of this week, it's affecting the user experience. And we need to update the API documentation before Friday's release - this is high priority. Oh, and someone should design the new onboarding screens for the next sprint. Arjun, didn't you work on UI designs last month? This can wait until next Monday. One more thing - we need to write unit tests for the payment module. This depends on the login bug fix being completed first, so let's plan this for Wednesday.\n",
        "\"\"\")\n",
        "\n",
        "outdir = \"/content/drive/MyDrive/meeting-task-assigner/sample_data\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "outfile = os.path.join(outdir, \"sample_meeting.mp3\")\n",
        "tts = gTTS(meeting_text)\n",
        "tts.save(outfile)\n",
        "print(\"Saved synthetic meeting audio to:\", outfile)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wLLID6csl6-",
        "outputId": "4013cea0-3f26-4571-c568-f4183bbbe859"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved synthetic meeting audio to: /content/drive/MyDrive/meeting-task-assigner/sample_data/sample_meeting.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/meeting-task-assigner\"\n",
        "SAMPLE_DIR = os.path.join(DRIVE_BASE, \"sample_data\")\n",
        "OUT_DIR = os.path.join(DRIVE_BASE, \"outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Choose the file to transcribe:\n",
        "# If you uploaded a file via Option A, replace filename below with that file's name\n",
        "# If you used Option B, the generated file is \"sample_meeting.mp3\"\n",
        "filename = None\n",
        "# auto-detect any audio file in sample_data if filename not manually set\n",
        "if not filename:\n",
        "    candidates = [p for p in os.listdir(SAMPLE_DIR) if p.lower().endswith(('.mp3','.wav','.m4a'))]\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\"No audio files found in sample_data. Upload one or run the generator.\")\n",
        "    filename = candidates[0]\n",
        "audio_path = os.path.join(SAMPLE_DIR, filename)\n",
        "print(\"Transcribing:\", audio_path)\n",
        "\n",
        "# Load model (choose 'small' for speed; change to 'base'/'medium' if you prefer)\n",
        "model = whisper.load_model(\"small\")   # change to \"base\" or \"medium\" if you want higher accuracy\n",
        "\n",
        "# If you encounter CUDA/FP16 errors, set fp16=False in transcribe\n",
        "result = model.transcribe(audio_path, fp16=False)\n",
        "transcript = result.get(\"text\",\"\").strip()\n",
        "\n",
        "# Save transcript with timestamp\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "outpath = os.path.join(OUT_DIR, f\"transcript_{ts}.txt\")\n",
        "with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(transcript)\n",
        "# Also create a stable file transcript.txt for pipeline use\n",
        "stable = os.path.join(OUT_DIR, \"transcript.txt\")\n",
        "with open(stable, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(transcript)\n",
        "\n",
        "print(\"Transcript saved to:\", outpath)\n",
        "print(\"Stable transcript path:\", stable)\n",
        "print(\"\\n--- Transcript preview (first 800 chars) ---\\n\")\n",
        "print(transcript[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-y9OcLTwGpC",
        "outputId": "c21d53e4-307a-4d14-9b12-16dc38b31198"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing: /content/drive/MyDrive/meeting-task-assigner/sample_data/sample_meeting.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:09<00:00, 50.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript saved to: /content/drive/MyDrive/meeting-task-assigner/outputs/transcript_20251129_040032.txt\n",
            "Stable transcript path: /content/drive/MyDrive/meeting-task-assigner/outputs/transcript.txt\n",
            "\n",
            "--- Transcript preview (first 800 chars) ---\n",
            "\n",
            "Hi everyone, let's discuss this week's priorities. Sockshee, we need someone to fix the critical login bug that users reported yesterday. This needs to be done by tomorrow evening since it's blocking users. Also, the database performance is really slow. Mo hit your good with backend optimization, right? We should tackle this by end of this week. It's affecting the user experience. And we need to update the API documentation before Friday's release. This is high priority. Oh, and someone should design the new onboarding screens for the next sprint. Arjun, didn't you work on UI designs last month? This can wait until next Monday. One more thing, we need to write unit tests for the payment module. This depends on the login bug fix being completed first. So let's plan this for Wednesday.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Step 3: NLP extraction + assignment\n",
        "import os, json, re\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import dateparser\n",
        "import spacy\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# --- Config / paths ---\n",
        "BASE = \"/content/drive/MyDrive/meeting-task-assigner\"\n",
        "TRANSCRIPT_PATH = os.path.join(BASE, \"outputs\", \"transcript.txt\")\n",
        "TEAM_PATH = os.path.join(BASE, \"sample_data\", \"team.json\")\n",
        "OUT_TASKS = os.path.join(BASE, \"outputs\", \"tasks.json\")\n",
        "\n",
        "# Load spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load transcript\n",
        "with open(TRANSCRIPT_PATH, encoding=\"utf-8\") as f:\n",
        "    transcript = f.read().strip()\n",
        "\n",
        "print(\"Transcript (first 400 chars):\\n\", transcript[:400], \"\\n---\\n\")\n",
        "\n",
        "# Load team\n",
        "with open(TEAM_PATH, encoding=\"utf-8\") as f:\n",
        "    team = json.load(f)\n",
        "\n",
        "team_names = [m[\"name\"] for m in team]\n",
        "\n",
        "# Helper functions\n",
        "PRIORITY_KEYWORDS = {\n",
        "    \"Critical\": [\"critical\", \"blocking\", \"urgent\", \"asap\"],\n",
        "    \"High\": [\"high priority\", \"high priority.\", \"high\", \"important\", \"before friday\", \"before friday's release\"],\n",
        "    \"Medium\": [\"medium\", \"next week\", \"next monday\", \"next monday.\", \"next sprint\", \"can wait\", \"plan\", \"by end of this week\", \"end of this week\"],\n",
        "    \"Low\": [\"low\", \"whenever\", \"nice to have\"]\n",
        "}\n",
        "\n",
        "TASK_VERB_KEYWORDS = [\"fix\", \"update\", \"design\", \"write\", \"optimize\", \"test\", \"implement\", \"create\", \"tackle\", \"plan\"]\n",
        "\n",
        "def detect_priority(text):\n",
        "    t = text.lower()\n",
        "    for p, kws in PRIORITY_KEYWORDS.items():\n",
        "        for kw in kws:\n",
        "            if kw in t:\n",
        "                return p\n",
        "    # heuristic: very soon deadlines -> bump priority\n",
        "    if any(x in t for x in [\"tomorrow\",\"by tomorrow\",\"today\",\"tonight\"]):\n",
        "        return \"Critical\"\n",
        "    return \"Medium\"\n",
        "\n",
        "def extract_deadline(text, ref_date=None):\n",
        "    if ref_date is None:\n",
        "        ref_date = datetime.now()\n",
        "    # Use dateparser.search.search_dates to find phrases\n",
        "    try:\n",
        "        results = dateparser.search.search_dates(text, settings={'RELATIVE_BASE': ref_date, 'PREFER_DATES_FROM': 'future'})\n",
        "    except Exception:\n",
        "        results = None\n",
        "    if results:\n",
        "        # prefer the first relevant-looking date (some phrases return time ranges)\n",
        "        phrase, dt = results[0]\n",
        "        # Convert to ISO if possible\n",
        "        try:\n",
        "            iso = dt.isoformat()\n",
        "            return iso, phrase\n",
        "        except Exception:\n",
        "            return str(dt), phrase\n",
        "    # heuristic catches\n",
        "    low = text.lower()\n",
        "    if \"end of this week\" in low or \"by end of this week\" in low:\n",
        "        # approximate as coming sunday of the same week\n",
        "        base = ref_date\n",
        "        days_to_sun = (6 - base.weekday()) % 7\n",
        "        dt = (base + timedelta(days=days_to_sun)).replace(hour=23, minute=59, second=0, microsecond=0)\n",
        "        return dt.isoformat(), \"end of this week\"\n",
        "    return None, None\n",
        "\n",
        "def extract_task_candidates(transcript):\n",
        "    doc = nlp(transcript)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    candidates = []\n",
        "    for s in sentences:\n",
        "        s_lower = s.lower()\n",
        "        # heuristics: look for task-like phrases\n",
        "        if re.search(r\"\\b(we need|someone should|please|let's|we should|should|we need to|we need someone|we need someone to)\\b\", s_lower):\n",
        "            candidates.append(s)\n",
        "            continue\n",
        "        if any(kw in s_lower for kw in TASK_VERB_KEYWORDS):\n",
        "            candidates.append(s)\n",
        "            continue\n",
        "    # dedupe while preserving order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for s in candidates:\n",
        "        if s not in seen:\n",
        "            out.append(s)\n",
        "            seen.add(s)\n",
        "    return out\n",
        "\n",
        "def assign_assignee(sentence, team_members, name_threshold=70):\n",
        "    # 1) explicit name fuzzy match\n",
        "    names = [m['name'] for m in team_members]\n",
        "    best = process.extractOne(sentence, names)\n",
        "    if best and best[1] >= name_threshold:\n",
        "        name = best[0]\n",
        "        return name, f\"explicit_mention (score={best[1]})\"\n",
        "    # 2) skill match: check if any skill keyword appears\n",
        "    s = sentence.lower()\n",
        "    for m in team_members:\n",
        "        for skill in m.get(\"skills\", []):\n",
        "            if skill.lower() in s:\n",
        "                return m[\"name\"], f\"skill_match:{skill}\"\n",
        "    # 3) keyword-to-role heuristics\n",
        "    if any(k in s for k in [\"ui\", \"onboarding\", \"design\", \"figma\", \"ux\", \"ui designs\", \"mobile design\", \"screens\"]):\n",
        "        # prefer UI/UX designer\n",
        "        for m in team_members:\n",
        "            if \"designer\" in m.get(\"role\",\"\").lower() or \"ui\" in m.get(\"role\",\"\").lower():\n",
        "                return m[\"name\"], \"heuristic_role_match:ui/ux\"\n",
        "    if any(k in s for k in [\"database\",\"db\",\"backend\",\"api\",\"performance\",\"optimiz\"]):\n",
        "        for m in team_members:\n",
        "            if \"backend\" in m.get(\"role\",\"\").lower() or \"engineer\" in m.get(\"role\",\"\").lower():\n",
        "                return m[\"name\"], \"heuristic_role_match:backend\"\n",
        "    if any(k in s for k in [\"test\",\"unit test\",\"testing\",\"qa\",\"automation\",\"payment module\"]):\n",
        "        for m in team_members:\n",
        "            if \"qa\" in m.get(\"role\",\"\").lower() or \"quality\" in m.get(\"role\",\"\").lower():\n",
        "                return m[\"name\"], \"heuristic_role_match:qa\"\n",
        "    return None, \"no_match\"\n",
        "\n",
        "def title_from_sentence(s):\n",
        "    # create a concise title by trimming to verb+object using a regex heuristic\n",
        "    s = s.strip()\n",
        "    # remove leading vocatives like \"Sakshi,\" or \"Arjun,\"\n",
        "    s = re.sub(r'^[A-Z][a-z]+,\\s*', '', s)\n",
        "    # attempt to capture verb phrase\n",
        "    m = re.search(r\"(fix|update|design|write|optimize|test|implement|create|tackle|plan)\\s+(.+)\", s, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        verb = m.group(1).capitalize()\n",
        "        obj = m.group(2)\n",
        "        title = f\"{verb} {obj}\"\n",
        "        title = title[:100]\n",
        "        return title\n",
        "    # fallback: first 60 chars\n",
        "    return s[:80]\n",
        "\n",
        "# --- Extract candidates ---\n",
        "candidates = extract_task_candidates(transcript)\n",
        "print(f\"Found {len(candidates)} candidate sentences for tasks.\\n\")\n",
        "for i,s in enumerate(candidates,1):\n",
        "    print(i, \"-\", s)\n",
        "\n",
        "# --- Build tasks ---\n",
        "meeting_date = datetime.now()  # use current notebook time as reference\n",
        "tasks = []\n",
        "id_counter = 1\n",
        "for sent in candidates:\n",
        "    title = title_from_sentence(sent)\n",
        "    priority = detect_priority(sent)\n",
        "    deadline_iso, deadline_phrase = extract_deadline(sent, ref_date=meeting_date)\n",
        "    assignee, reason_tag = assign_assignee(sent, team, name_threshold=65)  # slightly lower threshold for ASR errors\n",
        "    task = {\n",
        "        \"id\": id_counter,\n",
        "        \"title\": title,\n",
        "        \"description\": sent,\n",
        "        \"assigned_to\": assignee,\n",
        "        \"deadline\": deadline_iso if deadline_iso else (deadline_phrase if deadline_phrase else None),\n",
        "        \"priority\": priority,\n",
        "        \"dependencies\": [],\n",
        "        \"reasoning\": reason_tag\n",
        "    }\n",
        "    tasks.append(task)\n",
        "    id_counter += 1\n",
        "\n",
        "# --- Dependency detection (simple) ---\n",
        "# Link tasks that mention \"depends on\" or reference other tasks by keywords\n",
        "for t in tasks:\n",
        "    desc = t[\"description\"].lower()\n",
        "    if \"depends on\" in desc or \"depend\" in desc or \"after\" in desc:\n",
        "        # attempt to find referenced task by searching for keywords like 'login' or 'payment'\n",
        "        for other in tasks:\n",
        "            if other[\"id\"] == t[\"id\"]:\n",
        "                continue\n",
        "            # keyword match between descriptions\n",
        "            for token in [\"login\",\"payment\",\"database\",\"api\",\"onboarding\",\"unit test\",\"tests\",\"bug\"]:\n",
        "                if token in desc and token in other[\"description\"].lower():\n",
        "                    t[\"dependencies\"].append(other[\"id\"])\n",
        "                    break\n",
        "\n",
        "# --- Post-process: if assignee None, try skill-match across team more aggressively ---\n",
        "for t in tasks:\n",
        "    if not t[\"assigned_to\"]:\n",
        "        # look for top skill match\n",
        "        s = t[\"description\"].lower()\n",
        "        best_match = None\n",
        "        for m in team:\n",
        "            score = 0\n",
        "            for skill in m.get(\"skills\", []):\n",
        "                if skill.lower() in s:\n",
        "                    score += 1\n",
        "            # also check role keywords\n",
        "            if m.get(\"role\"):\n",
        "                if m[\"role\"].lower().split()[0] in s:\n",
        "                    score += 0.5\n",
        "            if score > 0 and (best_match is None or score > best_match[1]):\n",
        "                best_match = (m[\"name\"], score)\n",
        "        if best_match:\n",
        "            t[\"assigned_to\"] = best_match[0]\n",
        "            t[\"reasoning\"] = f\"skill_fallback ({best_match[1]:.1f})\"\n",
        "\n",
        "# --- Save tasks to disk ---\n",
        "output = {\n",
        "    \"meeting_file\": os.path.basename(TRANSCRIPT_PATH),\n",
        "    \"extracted_at\": meeting_date.isoformat(),\n",
        "    \"transcript_preview\": transcript[:800],\n",
        "    \"tasks\": tasks\n",
        "}\n",
        "with open(OUT_TASKS, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump(output, fh, indent=2)\n",
        "\n",
        "print(\"\\nSaved tasks to:\", OUT_TASKS)\n",
        "\n",
        "# --- Pretty print tasks table ---\n",
        "from prettytable import PrettyTable\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"id\",\"title\",\"assigned_to\",\"deadline\",\"priority\",\"dependencies\",\"reasoning\"]\n",
        "for t in tasks:\n",
        "    table.add_row([t[\"id\"], t[\"title\"][:30], t[\"assigned_to\"], t[\"deadline\"], t[\"priority\"], t[\"dependencies\"], t[\"reasoning\"]])\n",
        "print(\"\\nExtracted tasks:\\n\")\n",
        "print(table)\n",
        "\n",
        "# Also print JSON summary\n",
        "print(\"\\nJSON tasks summary (first 400 chars):\\n\")\n",
        "print(json.dumps(output, indent=2)[:400])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM_8sYI5xA0T",
        "outputId": "518cdd1a-f5ed-4932-85f6-96b480180e97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript (first 400 chars):\n",
            " Hi everyone, let's discuss this week's priorities. Sockshee, we need someone to fix the critical login bug that users reported yesterday. This needs to be done by tomorrow evening since it's blocking users. Also, the database performance is really slow. Mo hit your good with backend optimization, right? We should tackle this by end of this week. It's affecting the user experience. And we need to u \n",
            "---\n",
            "\n",
            "Found 9 candidate sentences for tasks.\n",
            "\n",
            "1 - Hi everyone, let's discuss this week's priorities.\n",
            "2 - Sockshee, we need someone to fix the critical login bug that users reported yesterday.\n",
            "3 - We should tackle this by end of this week.\n",
            "4 - And we need to update the API documentation before Friday's release.\n",
            "5 - Oh, and someone should design the new onboarding screens for the next sprint.\n",
            "6 - Arjun, didn't you work on UI designs last month?\n",
            "7 - One more thing, we need to write unit tests for the payment module.\n",
            "8 - This depends on the login bug fix being completed first.\n",
            "9 - So let's plan this for Wednesday.\n",
            "\n",
            "Saved tasks to: /content/drive/MyDrive/meeting-task-assigner/outputs/tasks.json\n",
            "\n",
            "Extracted tasks:\n",
            "\n",
            "+----+--------------------------------+-------------+---------------------+----------+--------------+------------------------------+\n",
            "| id |             title              | assigned_to |       deadline      | priority | dependencies |          reasoning           |\n",
            "+----+--------------------------------+-------------+---------------------+----------+--------------+------------------------------+\n",
            "| 1  | Hi everyone, let's discuss thi |     None    |         None        |  Medium  |      []      |           no_match           |\n",
            "| 2  | Fix the critical login bug tha |     None    |         None        | Critical |      []      |           no_match           |\n",
            "| 3  | Tackle this by end of this wee |     None    | 2025-11-30T23:59:00 |  Medium  |      []      |           no_match           |\n",
            "| 4  | Update the API documentation b |    Mohit    |         None        |   High   |      []      | heuristic_role_match:backend |\n",
            "| 5  | Design the new onboarding scre |    Arjun    |         None        |  Medium  |      []      |  heuristic_role_match:ui/ux  |\n",
            "| 6  | didn't you work on UI designs  |    Sakshi   |         None        |  Medium  |      []      |        skill_match:UI        |\n",
            "| 7  | Write unit tests for the payme |     Lata    |         None        |  Medium  |      []      |    skill_match:unit tests    |\n",
            "| 8  |   Fix being completed first.   |     None    |         None        |  Medium  |     [2]      |           no_match           |\n",
            "| 9  |    Plan this for Wednesday.    |     None    |         None        |  Medium  |      []      |           no_match           |\n",
            "+----+--------------------------------+-------------+---------------------+----------+--------------+------------------------------+\n",
            "\n",
            "JSON tasks summary (first 400 chars):\n",
            "\n",
            "{\n",
            "  \"meeting_file\": \"transcript.txt\",\n",
            "  \"extracted_at\": \"2025-11-29T04:04:04.423740\",\n",
            "  \"transcript_preview\": \"Hi everyone, let's discuss this week's priorities. Sockshee, we need someone to fix the critical login bug that users reported yesterday. This needs to be done by tomorrow evening since it's blocking users. Also, the database performance is really slow. Mo hit your good with backend optim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved extraction + assignment (run in Colab)\n",
        "import os, json, re\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import dateparser\n",
        "import spacy\n",
        "from fuzzywuzzy import process, fuzz\n",
        "from collections import defaultdict\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/meeting-task-assigner\"\n",
        "TRANSCRIPT_PATH = os.path.join(BASE, \"outputs\", \"transcript.txt\")\n",
        "TEAM_PATH = os.path.join(BASE, \"sample_data\", \"team.json\")\n",
        "OUT_TASKS = os.path.join(BASE, \"outputs\", \"tasks_refined.json\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open(TRANSCRIPT_PATH, encoding=\"utf-8\") as f:\n",
        "    transcript = f.read().strip()\n",
        "\n",
        "with open(TEAM_PATH, encoding=\"utf-8\") as f:\n",
        "    team = json.load(f)\n",
        "team_names = [m[\"name\"] for m in team]\n",
        "\n",
        "# Useful helpers\n",
        "PRIORITY_KEYWORDS = {\n",
        "    \"Critical\": [\"critical\", \"blocking\", \"urgent\", \"asap\"],\n",
        "    \"High\": [\"high priority\", \"high\", \"important\", \"before friday\", \"before friday's release\"],\n",
        "    \"Medium\": [\"medium\", \"next week\", \"next monday\", \"next sprint\", \"can wait\", \"plan\", \"end of this week\"],\n",
        "    \"Low\": [\"low\", \"whenever\", \"nice to have\"]\n",
        "}\n",
        "TASK_VERB_KEYWORDS = [\"fix\",\"update\",\"design\",\"write\",\"optimize\",\"test\",\"implement\",\"create\",\"tackle\",\"plan\",\"add\",\"remove\",\"investigate\",\"refactor\"]\n",
        "\n",
        "def detect_priority(text):\n",
        "    t = text.lower()\n",
        "    for p,kws in PRIORITY_KEYWORDS.items():\n",
        "        for kw in kws:\n",
        "            if kw in t:\n",
        "                return p\n",
        "    if any(x in t for x in [\"tomorrow\",\"by tomorrow\",\"today\",\"tonight\"]):\n",
        "        return \"Critical\"\n",
        "    return \"Medium\"\n",
        "\n",
        "def extract_date_from_text(text, ref_date=None):\n",
        "    if ref_date is None:\n",
        "        ref_date = datetime.now()\n",
        "    try:\n",
        "        results = dateparser.search.search_dates(text, settings={'RELATIVE_BASE': ref_date, 'PREFER_DATES_FROM':'future'})\n",
        "    except Exception:\n",
        "        results = None\n",
        "    if results:\n",
        "        phrase, dt = results[0]\n",
        "        return dt.isoformat(), phrase\n",
        "    # heuristics\n",
        "    low = text.lower()\n",
        "    if \"end of this week\" in low or \"by end of this week\" in low:\n",
        "        base = ref_date\n",
        "        days_to_sun = (6 - base.weekday()) % 7\n",
        "        dt = (base + timedelta(days=days_to_sun)).replace(hour=23, minute=59, second=0, microsecond=0)\n",
        "        return dt.isoformat(), \"end of this week\"\n",
        "    return None, None\n",
        "\n",
        "# --- REPLACE existing find_vocative_name with this improved function ---\n",
        "BAD_VOCATIVES = set([\"and\",\"but\",\"oh\",\"so\",\"also\",\"well\",\"hey\",\"hi\",\"hello\",\"one\",\"alright\"])\n",
        "\n",
        "def find_vocative_name(sentence):\n",
        "    # match \"Name,\" at start but ignore short/common words\n",
        "    m = re.match(r'^\\s*([A-Z][a-zA-Z]{2,})(?:\\s+|,)', sentence)\n",
        "    if m:\n",
        "        tok = m.group(1)\n",
        "        if tok.lower() not in BAD_VOCATIVES and len(tok) > 2:\n",
        "            return tok\n",
        "    # match patterns like \"Sakshi,\" or \"Arjun,\" later in sentence beginning\n",
        "    m2 = re.match(r'^\\s*([A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{1,})?),', sentence)\n",
        "    if m2:\n",
        "        tok = m2.group(1).replace(\" \", \"\")\n",
        "        if tok.lower() not in BAD_VOCATIVES and len(tok) > 2:\n",
        "            return tok\n",
        "    return None\n",
        "\n",
        "\n",
        "def normalize_name_token(tok):\n",
        "    # remove non-letters, join split pieces like \"Mo hit\" -> \"Mohit\"\n",
        "    s = re.sub(r'[^A-Za-z]', '', tok)\n",
        "    return s\n",
        "\n",
        "def best_team_name_match(token, names, threshold=65):\n",
        "    if not token:\n",
        "        return None, 0\n",
        "    token_clean = normalize_name_token(token)\n",
        "    # Try direct fuzzy\n",
        "    best = process.extractOne(token_clean, names, scorer=fuzz.token_sort_ratio)\n",
        "    if best and best[1] >= threshold:\n",
        "        return best[0], best[1]\n",
        "    # try join-split variants (e.g., \"Mo hit\" -> \"Mohit\")\n",
        "    joined = token.replace(\" \", \"\")\n",
        "    best2 = process.extractOne(joined, names, scorer=fuzz.token_sort_ratio)\n",
        "    if best2 and best2[1] >= threshold:\n",
        "        return best2[0], best2[1]\n",
        "    # lower threshold fallback if token is short but close\n",
        "    if best and best[1] >= (threshold - 10):\n",
        "        return best[0], best[1]\n",
        "    return None, 0\n",
        "\n",
        "def extract_task_sentences(transcript):\n",
        "    doc = nlp(transcript)\n",
        "    sents = [sent.text.strip() for sent in doc.sents]\n",
        "    candidates_idx = []\n",
        "    for i,s in enumerate(sents):\n",
        "        sl = s.lower()\n",
        "        if re.search(r\"\\b(we need|someone should|please|let's|we should|should|we need to|we need someone|we need someone to|one more thing)\\b\", sl):\n",
        "            candidates_idx.append(i)\n",
        "            continue\n",
        "        if any(kw in sl for kw in TASK_VERB_KEYWORDS):\n",
        "            candidates_idx.append(i)\n",
        "            continue\n",
        "    # dedupe preserving order\n",
        "    seen=set(); out=[]\n",
        "    for i in candidates_idx:\n",
        "        if i not in seen:\n",
        "            out.append(i); seen.add(i)\n",
        "    return sents, out\n",
        "\n",
        "# Merge cluster: attach short referential sentences and deadline-bearing neighbors\n",
        "def build_clusters(sents, candidate_idxs):\n",
        "    clusters = []  # list of dicts {idxs:list, text: str}\n",
        "    i = 0\n",
        "    for idx in candidate_idxs:\n",
        "        # start cluster at idx, then attach following sentences if they are referential/short or contain date words\n",
        "        cluster_idxs = [idx]\n",
        "        # look ahead up to 2 sentences\n",
        "        for j in range(1,3):\n",
        "            if idx + j >= len(sents):\n",
        "                break\n",
        "            next_s = sents[idx+j].strip()\n",
        "            low = next_s.lower()\n",
        "            # attach if it's referential or contains deadlines/dependency words\n",
        "            if len(next_s.split()) < 6 or any(w in low for w in [\"tomorrow\",\"wednesday\",\"friday\",\"monday\",\"by\",\"depends on\",\"depends\",\"depend\",\"so let's\",\"plan\",\"this needs\",\"this is\",\"it's\",\"it's affecting\",\"end of this week\"]):\n",
        "                cluster_idxs.append(idx+j)\n",
        "            else:\n",
        "                break\n",
        "        cluster_text = \" \".join([sents[k] for k in cluster_idxs])\n",
        "        clusters.append({\"idxs\":cluster_idxs, \"text\": cluster_text, \"first_idx\": idx})\n",
        "    return clusters\n",
        "\n",
        "# Build candidates\n",
        "sents, candidate_idxs = extract_task_sentences(transcript)\n",
        "clusters = build_clusters(sents, candidate_idxs)\n",
        "\n",
        "# For debug: show clusters\n",
        "print(f\"Found {len(clusters)} task clusters.\\n\")\n",
        "for i,c in enumerate(clusters,1):\n",
        "    print(i,\"(sent idxs:\",c['idxs'],\") ->\", c['text'])\n",
        "\n",
        "# Assignment and extraction\n",
        "tasks = []\n",
        "idc = 1\n",
        "meeting_date = datetime.now()\n",
        "for c in clusters:\n",
        "    text = c[\"text\"]\n",
        "    # detect vocative\n",
        "    voc = find_vocative_name(sents[c[\"first_idx\"]])\n",
        "    assignee = None\n",
        "    reasoning = None\n",
        "    if voc:\n",
        "        name, score = best_team_name_match(voc, team_names, threshold=60)\n",
        "        if name:\n",
        "            assignee = name\n",
        "            reasoning = f\"vocative_explicit ({voc}, score={score})\"\n",
        "    # if not assigned, try fuzzy match on full cluster text for names\n",
        "    if not assignee:\n",
        "        best = process.extractOne(text, team_names, scorer=fuzz.token_sort_ratio)\n",
        "        if best and best[1] >= 70:\n",
        "            assignee = best[0]\n",
        "            reasoning = f\"explicit_mention_full ({best[1]})\"\n",
        "    # fallback skill matching\n",
        "    if not assignee:\n",
        "        tl = text.lower()\n",
        "        for m in team:\n",
        "            for skill in m.get(\"skills\",[]):\n",
        "                if skill.lower() in tl:\n",
        "                    assignee = m[\"name\"]\n",
        "                    reasoning = f\"skill_match:{skill}\"\n",
        "                    break\n",
        "            if assignee: break\n",
        "    # heuristic role keywords\n",
        "    if not assignee:\n",
        "        tl = text.lower()\n",
        "        if any(k in tl for k in [\"ui\",\"onboarding\",\"design\",\"figma\",\"ux\",\"screens\"]):\n",
        "            for m in team:\n",
        "                if \"designer\" in m.get(\"role\",\"\").lower() or \"ui\" in m.get(\"role\",\"\").lower():\n",
        "                    assignee = m[\"name\"]; reasoning = \"heuristic_role:ui/ux\"; break\n",
        "        if not assignee and any(k in tl for k in [\"database\",\"db\",\"backend\",\"api\",\"performance\",\"optimiz\"]):\n",
        "            for m in team:\n",
        "                if \"backend\" in m.get(\"role\",\"\").lower() or \"engineer\" in m.get(\"role\",\"\").lower():\n",
        "                    assignee = m[\"name\"]; reasoning = \"heuristic_role:backend\"; break\n",
        "        if not assignee and any(k in tl for k in [\"test\",\"unit test\",\"testing\",\"qa\",\"automation\",\"payment\"]):\n",
        "            for m in team:\n",
        "                if \"qa\" in m.get(\"role\",\"\").lower() or \"quality\" in m.get(\"role\",\"\").lower():\n",
        "                    assignee = m[\"name\"]; reasoning = \"heuristic_role:qa\"; break\n",
        "\n",
        "    # priority & deadline\n",
        "    priority = detect_priority(text)\n",
        "    deadline_iso, deadline_phrase = extract_date_from_text(text, ref_date=meeting_date)\n",
        "    # build title\n",
        "    # remove leading vocative tokens for title readability\n",
        "    title_text = re.sub(r'^[A-Z][a-zA-Z]+\\s*,\\s*', '', text)\n",
        "    m = re.search(r\"(fix|update|design|write|optimi(?:z|s)e|test|implement|create|tackle|plan)\\s+(.+?)(?:\\.|$)\", title_text, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        title = f\"{m.group(1).capitalize()} {m.group(2)}\"\n",
        "        title = title[:120]\n",
        "    else:\n",
        "        title = title_text[:100]\n",
        "\n",
        "    tasks.append({\n",
        "        \"id\": idc,\n",
        "        \"title\": title,\n",
        "        \"description\": text,\n",
        "        \"assigned_to\": assignee,\n",
        "        \"deadline\": deadline_iso if deadline_iso else (deadline_phrase if deadline_phrase else None),\n",
        "        \"deadline_phrase\": deadline_phrase,\n",
        "        \"priority\": priority,\n",
        "        \"dependencies\": [],\n",
        "        \"reasoning\": reasoning or \"no_match\"\n",
        "    })\n",
        "    idc += 1\n",
        "\n",
        "# Dependency linking by similarity of keywords\n",
        "for t in tasks:\n",
        "    desc = t[\"description\"].lower()\n",
        "    if any(k in desc for k in [\"depend\", \"depends on\", \"after\"]):\n",
        "        for other in tasks:\n",
        "            if other[\"id\"] == t[\"id\"]:\n",
        "                continue\n",
        "            # try to match by important tokens\n",
        "            for token in [\"login\",\"payment\",\"database\",\"api\",\"onboarding\",\"unit test\",\"tests\",\"bug\",\"login bug\",\"payment module\"]:\n",
        "                if token in desc and token in other[\"description\"].lower():\n",
        "                    t[\"dependencies\"].append(other[\"id\"])\n",
        "                    break\n",
        "\n",
        "# Save refined output\n",
        "output = {\n",
        "    \"meeting_file\": os.path.basename(TRANSCRIPT_PATH),\n",
        "    \"extracted_at\": meeting_date.isoformat(),\n",
        "    \"transcript_preview\": transcript[:1000],\n",
        "    \"tasks\": tasks\n",
        "}\n",
        "with open(OUT_TASKS, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump(output, fh, indent=2)\n",
        "\n",
        "# pretty print\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable()\n",
        "t.field_names = [\"id\",\"title\",\"assigned_to\",\"deadline\",\"priority\",\"dependencies\",\"reasoning\"]\n",
        "for x in tasks:\n",
        "    t.add_row([x[\"id\"], x[\"title\"][:40], x[\"assigned_to\"], x[\"deadline\"], x[\"priority\"], x[\"dependencies\"], x[\"reasoning\"]])\n",
        "print(\"\\nRefined extracted tasks:\\n\")\n",
        "print(t)\n",
        "print(\"\\nSaved refined tasks to:\", OUT_TASKS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLFHTKWlyFUy",
        "outputId": "d55d653f-b459-4952-c2dd-e4777882192b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 task clusters.\n",
            "\n",
            "1 (sent idxs: [0] ) -> Hi everyone, let's discuss this week's priorities.\n",
            "2 (sent idxs: [1, 2] ) -> Sockshee, we need someone to fix the critical login bug that users reported yesterday. This needs to be done by tomorrow evening since it's blocking users.\n",
            "3 (sent idxs: [5, 6, 7] ) -> We should tackle this by end of this week. It's affecting the user experience. And we need to update the API documentation before Friday's release.\n",
            "4 (sent idxs: [7, 8] ) -> And we need to update the API documentation before Friday's release. This is high priority.\n",
            "5 (sent idxs: [9] ) -> Oh, and someone should design the new onboarding screens for the next sprint.\n",
            "6 (sent idxs: [10, 11] ) -> Arjun, didn't you work on UI designs last month? This can wait until next Monday.\n",
            "7 (sent idxs: [12, 13, 14] ) -> One more thing, we need to write unit tests for the payment module. This depends on the login bug fix being completed first. So let's plan this for Wednesday.\n",
            "8 (sent idxs: [13, 14] ) -> This depends on the login bug fix being completed first. So let's plan this for Wednesday.\n",
            "9 (sent idxs: [14] ) -> So let's plan this for Wednesday.\n",
            "\n",
            "Refined extracted tasks:\n",
            "\n",
            "+----+------------------------------------------+-------------+---------------------+----------+--------------+----------------------------------------+\n",
            "| id |                  title                   | assigned_to |       deadline      | priority | dependencies |               reasoning                |\n",
            "+----+------------------------------------------+-------------+---------------------+----------+--------------+----------------------------------------+\n",
            "| 1  | Hi everyone, let's discuss this week's p |     None    |         None        |  Medium  |      []      |                no_match                |\n",
            "| 2  | Fix the critical login bug that users re |    Sakshi   |         None        | Critical |      []      | vocative_explicit (Sockshee, score=57) |\n",
            "| 3  |     Tackle this by end of this week      |    Mohit    | 2025-11-30T23:59:00 |   High   |      []      |         heuristic_role:backend         |\n",
            "| 4  | Update the API documentation before Frid |    Mohit    |         None        |   High   |      []      |         heuristic_role:backend         |\n",
            "| 5  | Design the new onboarding screens for th |    Arjun    |         None        |  Medium  |      []      |          heuristic_role:ui/ux          |\n",
            "| 6  | didn't you work on UI designs last month |    Arjun    |         None        |  Medium  |      []      |  vocative_explicit (Arjun, score=100)  |\n",
            "| 7  | Write unit tests for the payment module  |     Lata    |         None        |  Medium  |    [2, 8]    |         skill_match:unit tests         |\n",
            "| 8  |        Fix being completed first         |     None    |         None        |  Medium  |    [2, 7]    |                no_match                |\n",
            "| 9  |         Plan this for Wednesday          |     None    |         None        |  Medium  |      []      |                no_match                |\n",
            "+----+------------------------------------------+-------------+---------------------+----------+--------------+----------------------------------------+\n",
            "\n",
            "Saved refined tasks to: /content/drive/MyDrive/meeting-task-assigner/outputs/tasks_refined.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create test file\n",
        "cat > /content/drive/MyDrive/meeting-task-assigner/tests/test_pipeline_basic.py <<'PY'\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/meeting-task-assigner\")\n",
        "OUT = BASE / \"outputs\" / \"tasks_refined.json\"\n",
        "\n",
        "def test_tasks_refined_exists():\n",
        "    assert OUT.exists(), \"tasks_refined.json missing; run pipeline first\"\n",
        "\n",
        "def test_expected_task_count():\n",
        "    data = json.loads(OUT.read_text(encoding='utf-8'))\n",
        "    tasks = data['tasks']\n",
        "    # We'll expect at least 6 useful tasks from this meeting\n",
        "    assert len(tasks) >= 6, f\"Expected at least 6 tasks, found {len(tasks)}\"\n",
        "\n",
        "def test_core_assignments_present():\n",
        "    data = json.loads(OUT.read_text(encoding='utf-8'))\n",
        "    names = set(t.get('assigned_to') for t in data['tasks'] if t.get('assigned_to'))\n",
        "    # Ensure key team members appear as assignees somewhere\n",
        "    assert \"Sakshi\" in names, \"Sakshi should be assigned (login bug)\"\n",
        "    assert \"Mohit\" in names, \"Mohit should be assigned (DB/API)\"\n",
        "    assert \"Lata\" in names, \"Lata should be assigned (unit tests)\"\n",
        "    assert \"Arjun\" in names, \"Arjun should be assigned (UI tasks)\"\n",
        "PY\n",
        "\n",
        "# run pytest (quiet)\n",
        "pytest -q /content/drive/MyDrive/meeting-task-assigner/tests/test_pipeline_basic.py -q || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYvgQfUqycDa",
        "outputId": "9b6368f8-a2fa-4b21-d96c-63f1b4fb7242"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...                                                                      [100%]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXL5KOYcitLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}